{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c3c4618-6b5b-4c87-83ab-5581489cebce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker\n",
    "dbutils.notebook.exit(\"Restarting Python to complete installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6d380aa-dc4f-4bec-a812-d8eb7c79d818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"HealthcareClaimsGenerator\").getOrCreate()\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS healthcare_claims\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0b8847c-cd33-4c26-8be3-4f7c3b228d88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating patient data...\nGenerating provider data...\nGenerating insurance data...\nGenerating claims data...\nGenerating claim details...\nSaving to Delta tables...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3035526575224895>, line 453\u001B[0m\n",
       "\u001B[1;32m    451\u001B[0m \u001B[38;5;66;03m# Save data to Delta tables\u001B[39;00m\n",
       "\u001B[1;32m    452\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSaving to Delta tables...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m--> 453\u001B[0m patients_df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhealthcare_claims.patients\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    454\u001B[0m providers_df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhealthcare_claims.providers\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    455\u001B[0m insurance_df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhealthcare_claims.insurance\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/readwriter.py:713\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m    711\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_name \u001B[38;5;241m=\u001B[39m name\n",
       "\u001B[1;32m    712\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_save_method \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msave_as_table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m--> 713\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n",
       "\u001B[1;32m    714\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n",
       "\u001B[1;32m    715\u001B[0m )\n",
       "\u001B[1;32m    716\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1297\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1295\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1296\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1297\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1298\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1299\u001B[0m )\n",
       "\u001B[1;32m   1300\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1301\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1755\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   1752\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   1754\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 1755\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   1756\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   1757\u001B[0m     ):\n",
       "\u001B[1;32m   1758\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1759\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1731\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   1729\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   1730\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1731\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2047\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2045\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2046\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2047\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2048\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   2049\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2149\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2134\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n",
       "\u001B[1;32m   2135\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2136\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2145\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://docs.databricks.com/en/release-notes/serverless.html.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2146\u001B[0m                 )\n",
       "\u001B[1;32m   2147\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2149\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2150\u001B[0m                 info,\n",
       "\u001B[1;32m   2151\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2152\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2153\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2154\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2156\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2157\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [SCHEMA_NOT_FOUND] The schema `workspace.healthcare_claims` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.\n",
       "To tolerate the error on drop use DROP SCHEMA IF EXISTS. SQLSTATE: 42704\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$createStagingTable$2(ManagedCatalogClientImpl.scala:1291)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapException$2(ManagedCatalogClientImpl.scala:6825)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapException$1(ManagedCatalogClientImpl.scala:6824)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:38)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:36)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:220)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:6805)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.createStagingTable(ManagedCatalogClientImpl.scala:1265)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$createStagingTable$1(ManagedCatalogClientImpl.scala:1256)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapException$2(ManagedCatalogClientImpl.scala:6825)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapException$1(ManagedCatalogClientImpl.scala:6824)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:38)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:36)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:220)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:6805)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.createStagingTable(ManagedCatalogClientImpl.scala:1254)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.createStagingTable(ManagedCatalogCommon.scala:943)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$createStagingTable$1(ProfiledManagedCatalog.scala:211)\n",
       "\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1253)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:63)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:62)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.createStagingTable(ProfiledManagedCatalog.scala:211)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.defaultTablePathImpl(ManagedCatalogSessionCatalog.scala:1284)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.defaultTablePath(ManagedCatalogSessionCatalog.scala:1297)\n",
       "\tat com.databricks.sql.transaction.tahoe.coordinatedcommits.UCCommitCoordinatorHelper$.getDefaultLocationAndPropertiesWithoutUCCC$1(UCCommitCoordinatorClientEdge.scala:312)\n",
       "\tat com.databricks.sql.transaction.tahoe.coordinatedcommits.UCCommitCoordinatorHelper$.$anonfun$getDefaultLocationAndUpdatedProperties$6(UCCommitCoordinatorClientEdge.scala:381)\n",
       "\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:62)\n",
       "\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:59)\n",
       "\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:140)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordFrameProfileWithDmqTag$1(DeltaLogging.scala:304)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfileWithDmqTag(DeltaLogging.scala:304)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfileWithDmqTag$(DeltaLogging.scala:303)\n",
       "\tat com.databricks.sql.transaction.tahoe.coordinatedcommits.UCCommitCoordinatorHelper$.recordFrameProfileWithDmqTag(UCCommitCoordinatorClientEdge.scala:292)\n",
       "\tat com.databricks.sql.transaction.tahoe.coordinatedcommits.UCCommitCoordinatorHelper$.getDefaultLocationAndUpdatedProperties(UCCommitCoordinatorClientEdge.scala:369)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:280)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:316)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:314)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:123)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:181)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1251)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:316)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:314)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:123)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1210)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:310)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$6(WriteToDataSourceV2Exec.scala:751)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$.handleConcurrentCreateExceptions(WriteToDataSourceV2Exec.scala:72)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:751)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1621)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:738)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:755)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:730)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:252)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:315)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:404)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:404)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:211)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:404)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:465)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:750)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:335)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1360)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:206)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:687)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:400)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1224)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:396)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:346)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:394)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:448)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:443)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:505)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:505)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:379)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:375)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:481)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:443)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:436)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:443)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:302)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1676)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1737)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:473)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1059)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:763)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:694)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3558)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3060)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:402)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:289)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:210)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:401)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1360)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:401)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:400)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:210)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:129)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:628)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:628)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[SCHEMA_NOT_FOUND] The schema `workspace.healthcare_claims` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.\nTo tolerate the error on drop use DROP SCHEMA IF EXISTS. SQLSTATE: 42704\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$createStagingTable$2(ManagedCatalogClientImpl.scala:1291)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapException$2(ManagedCatalogClientImpl.scala:6825)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapException$1(ManagedCatalogClientImpl.scala:6824)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:38)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:36)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:220)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:6805)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.createStagingTable(ManagedCatalogClientImpl.scala:1265)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$createStagingTable$1(ManagedCatalogClientImpl.scala:1256)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapException$2(ManagedCatalogClientImpl.scala:6825)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapException$1(ManagedCatalogClientImpl.scala:6824)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:38)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:36)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:220)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:6805)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.createStagingTable(ManagedCatalogClientImpl.scala:1254)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.createStagingTable(ManagedCatalogCommon.scala:943)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$createStagingTable$1(ProfiledManagedCatalog.scala:211)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1253)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:62)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.createStagingTable(ProfiledManagedCatalog.scala:211)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.defaultTablePathImpl(ManagedCatalogSessionCatalog.scala:1284)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.defaultTablePath(ManagedCatalogSessionCatalog.scala:1297)\n\tat com.databricks.sql.transaction.tahoe.coordinatedcommits.UCCommitCoordinatorHelper$.getDefaultLocationAndPropertiesWithoutUCCC$1(UCCommitCoordinatorClientEdge.scala:312)\n\tat com.databricks.sql.transaction.tahoe.coordinatedcommits.UCCommitCoordinatorHelper$.$anonfun$getDefaultLocationAndUpdatedProperties$6(UCCommitCoordinatorClientEdge.scala:381)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:62)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:59)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:140)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordFrameProfileWithDmqTag$1(DeltaLogging.scala:304)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfileWithDmqTag(DeltaLogging.scala:304)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfileWithDmqTag$(DeltaLogging.scala:303)\n\tat com.databricks.sql.transaction.tahoe.coordinatedcommits.UCCommitCoordinatorHelper$.recordFrameProfileWithDmqTag(UCCommitCoordinatorClientEdge.scala:292)\n\tat com.databricks.sql.transaction.tahoe.coordinatedcommits.UCCommitCoordinatorHelper$.getDefaultLocationAndUpdatedProperties(UCCommitCoordinatorClientEdge.scala:369)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:280)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:316)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:314)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:123)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:181)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1251)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:316)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:314)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:123)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1210)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:310)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$6(WriteToDataSourceV2Exec.scala:751)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$.handleConcurrentCreateExceptions(WriteToDataSourceV2Exec.scala:72)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:751)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1621)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:738)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:755)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:730)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:252)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:315)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:404)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:404)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:211)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:404)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:465)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:750)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:335)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1360)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:206)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:687)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:400)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1224)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:396)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:346)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:394)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:448)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:443)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:505)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:505)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:41)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:379)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:375)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:41)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:41)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:481)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:443)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:436)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:443)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:302)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1676)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1737)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:307)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:473)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1059)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:763)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:694)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3558)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3060)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:402)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:289)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:210)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:401)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1360)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:401)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:400)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:210)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:129)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:628)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:628)"
       },
       "metadata": {
        "errorSummary": "[SCHEMA_NOT_FOUND] The schema `workspace.healthcare_claims` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.\nTo tolerate the error on drop use DROP SCHEMA IF EXISTS. SQLSTATE: 42704"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "SCHEMA_NOT_FOUND",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "sqlState": "42704",
        "stackTrace": "org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$createStagingTable$2(ManagedCatalogClientImpl.scala:1291)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapException$2(ManagedCatalogClientImpl.scala:6825)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapException$1(ManagedCatalogClientImpl.scala:6824)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:38)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:36)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:220)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:6805)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.createStagingTable(ManagedCatalogClientImpl.scala:1265)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$createStagingTable$1(ManagedCatalogClientImpl.scala:1256)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapException$2(ManagedCatalogClientImpl.scala:6825)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapException$1(ManagedCatalogClientImpl.scala:6824)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:38)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:36)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:220)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:6805)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.createStagingTable(ManagedCatalogClientImpl.scala:1254)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.createStagingTable(ManagedCatalogCommon.scala:943)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$createStagingTable$1(ProfiledManagedCatalog.scala:211)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1253)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:62)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.createStagingTable(ProfiledManagedCatalog.scala:211)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.defaultTablePathImpl(ManagedCatalogSessionCatalog.scala:1284)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.defaultTablePath(ManagedCatalogSessionCatalog.scala:1297)\n\tat com.databricks.sql.transaction.tahoe.coordinatedcommits.UCCommitCoordinatorHelper$.getDefaultLocationAndPropertiesWithoutUCCC$1(UCCommitCoordinatorClientEdge.scala:312)\n\tat com.databricks.sql.transaction.tahoe.coordinatedcommits.UCCommitCoordinatorHelper$.$anonfun$getDefaultLocationAndUpdatedProperties$6(UCCommitCoordinatorClientEdge.scala:381)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:62)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:59)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:140)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordFrameProfileWithDmqTag$1(DeltaLogging.scala:304)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfileWithDmqTag(DeltaLogging.scala:304)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfileWithDmqTag$(DeltaLogging.scala:303)\n\tat com.databricks.sql.transaction.tahoe.coordinatedcommits.UCCommitCoordinatorHelper$.recordFrameProfileWithDmqTag(UCCommitCoordinatorClientEdge.scala:292)\n\tat com.databricks.sql.transaction.tahoe.coordinatedcommits.UCCommitCoordinatorHelper$.getDefaultLocationAndUpdatedProperties(UCCommitCoordinatorClientEdge.scala:369)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:280)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:316)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:314)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:123)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:181)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1251)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:316)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:314)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:123)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1210)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:310)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$6(WriteToDataSourceV2Exec.scala:751)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$.handleConcurrentCreateExceptions(WriteToDataSourceV2Exec.scala:72)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:751)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1621)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:738)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:755)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:730)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:252)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:315)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:404)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:404)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:211)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:404)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:465)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:750)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:335)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1360)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:206)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:687)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:400)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1224)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:396)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:346)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:394)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:448)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:443)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:505)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:505)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:41)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:379)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:375)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:41)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:41)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:481)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:443)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:436)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:443)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:302)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1676)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1737)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:307)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:473)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1059)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:763)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:694)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3558)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3060)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:402)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:289)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:210)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:401)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1360)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:401)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:400)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:210)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:129)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:628)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:628)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-3035526575224895>, line 453\u001B[0m\n\u001B[1;32m    451\u001B[0m \u001B[38;5;66;03m# Save data to Delta tables\u001B[39;00m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSaving to Delta tables...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 453\u001B[0m patients_df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhealthcare_claims.patients\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    454\u001B[0m providers_df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhealthcare_claims.providers\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    455\u001B[0m insurance_df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhealthcare_claims.insurance\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/readwriter.py:713\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m    711\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_name \u001B[38;5;241m=\u001B[39m name\n\u001B[1;32m    712\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_save_method \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msave_as_table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 713\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n\u001B[1;32m    714\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n\u001B[1;32m    715\u001B[0m )\n\u001B[1;32m    716\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1297\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1295\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1296\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1297\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1298\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1299\u001B[0m )\n\u001B[1;32m   1300\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1301\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1755\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   1752\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1754\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1755\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   1756\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   1757\u001B[0m     ):\n\u001B[1;32m   1758\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1759\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1731\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1729\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1730\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1731\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2047\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2045\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2046\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2047\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2048\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2049\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2149\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2134\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[1;32m   2135\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2136\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2145\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://docs.databricks.com/en/release-notes/serverless.html.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2146\u001B[0m                 )\n\u001B[1;32m   2147\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n\u001B[0;32m-> 2149\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2150\u001B[0m                 info,\n\u001B[1;32m   2151\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2152\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2153\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2154\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2156\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2157\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [SCHEMA_NOT_FOUND] The schema `workspace.healthcare_claims` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.\nTo tolerate the error on drop use DROP SCHEMA IF EXISTS. SQLSTATE: 42704\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$createStagingTable$2(ManagedCatalogClientImpl.scala:1291)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapException$2(ManagedCatalogClientImpl.scala:6825)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapException$1(ManagedCatalogClientImpl.scala:6824)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:38)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:36)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:220)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:6805)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.createStagingTable(ManagedCatalogClientImpl.scala:1265)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$createStagingTable$1(ManagedCatalogClientImpl.scala:1256)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapException$2(ManagedCatalogClientImpl.scala:6825)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapException$1(ManagedCatalogClientImpl.scala:6824)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:38)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:36)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:220)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:6805)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.createStagingTable(ManagedCatalogClientImpl.scala:1254)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.createStagingTable(ManagedCatalogCommon.scala:943)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$createStagingTable$1(ProfiledManagedCatalog.scala:211)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1253)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:62)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.createStagingTable(ProfiledManagedCatalog.scala:211)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.defaultTablePathImpl(ManagedCatalogSessionCatalog.scala:1284)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.defaultTablePath(ManagedCatalogSessionCatalog.scala:1297)\n\tat com.databricks.sql.transaction.tahoe.coordinatedcommits.UCCommitCoordinatorHelper$.getDefaultLocationAndPropertiesWithoutUCCC$1(UCCommitCoordinatorClientEdge.scala:312)\n\tat com.databricks.sql.transaction.tahoe.coordinatedcommits.UCCommitCoordinatorHelper$.$anonfun$getDefaultLocationAndUpdatedProperties$6(UCCommitCoordinatorClientEdge.scala:381)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:62)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:59)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:140)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordFrameProfileWithDmqTag$1(DeltaLogging.scala:304)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfileWithDmqTag(DeltaLogging.scala:304)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfileWithDmqTag$(DeltaLogging.scala:303)\n\tat com.databricks.sql.transaction.tahoe.coordinatedcommits.UCCommitCoordinatorHelper$.recordFrameProfileWithDmqTag(UCCommitCoordinatorClientEdge.scala:292)\n\tat com.databricks.sql.transaction.tahoe.coordinatedcommits.UCCommitCoordinatorHelper$.getDefaultLocationAndUpdatedProperties(UCCommitCoordinatorClientEdge.scala:369)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:280)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:316)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:314)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:123)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:181)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1251)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:316)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:314)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:123)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1210)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:310)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$6(WriteToDataSourceV2Exec.scala:751)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$.handleConcurrentCreateExceptions(WriteToDataSourceV2Exec.scala:72)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:751)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1621)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:738)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:755)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:730)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:252)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:315)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:404)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:404)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:211)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:404)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:465)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:750)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:335)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1360)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:206)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:687)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:400)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1224)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:396)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:346)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:394)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:448)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:443)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:505)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:505)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:41)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:379)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:375)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:41)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:41)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:481)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:443)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:436)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:443)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:302)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1676)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1737)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:307)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:473)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1059)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:763)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:694)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3558)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3060)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:402)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:289)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:210)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:401)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1360)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:401)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:400)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:210)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:129)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:628)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:628)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Synthetic Healthcare Claims Data Generator\n",
    "# For use in Databricks\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import random\n",
    "import datetime\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize Faker to generate realistic data\n",
    "fake = Faker()\n",
    "Faker.seed(42)  # For reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"HealthcareClaimsGenerator\").getOrCreate()\n",
    "\n",
    "# Define schemas for our data\n",
    "patient_schema = StructType([\n",
    "    StructField(\"patient_id\", StringType(), False),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"date_of_birth\", DateType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip_code\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"insurance_id\", StringType(), True),\n",
    "    StructField(\"created_date\", TimestampType(), True),\n",
    "    StructField(\"updated_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "provider_schema = StructType([\n",
    "    StructField(\"provider_id\", StringType(), False),\n",
    "    StructField(\"provider_name\", StringType(), True),\n",
    "    StructField(\"provider_type\", StringType(), True),\n",
    "    StructField(\"npi_number\", StringType(), True),\n",
    "    StructField(\"specialty\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip_code\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"created_date\", TimestampType(), True),\n",
    "    StructField(\"updated_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "insurance_schema = StructType([\n",
    "    StructField(\"insurance_id\", StringType(), False),\n",
    "    StructField(\"insurance_name\", StringType(), True),\n",
    "    StructField(\"plan_type\", StringType(), True),\n",
    "    StructField(\"plan_number\", StringType(), True),\n",
    "    StructField(\"contact_phone\", StringType(), True),\n",
    "    StructField(\"contact_email\", StringType(), True),\n",
    "    StructField(\"created_date\", TimestampType(), True),\n",
    "    StructField(\"updated_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "claims_schema = StructType([\n",
    "    StructField(\"claim_id\", StringType(), False),\n",
    "    StructField(\"patient_id\", StringType(), True),\n",
    "    StructField(\"provider_id\", StringType(), True),\n",
    "    StructField(\"insurance_id\", StringType(), True),\n",
    "    StructField(\"claim_date\", DateType(), True),\n",
    "    StructField(\"admission_date\", DateType(), True),\n",
    "    StructField(\"discharge_date\", DateType(), True),\n",
    "    StructField(\"claim_type\", StringType(), True),\n",
    "    StructField(\"diagnosis_codes\", ArrayType(StringType()), True),\n",
    "    StructField(\"procedure_codes\", ArrayType(StringType()), True),\n",
    "    StructField(\"place_of_service\", StringType(), True),\n",
    "    StructField(\"total_charge\", DoubleType(), True),\n",
    "    StructField(\"copay_amount\", DoubleType(), True),\n",
    "    StructField(\"insurance_paid\", DoubleType(), True),\n",
    "    StructField(\"patient_responsibility\", DoubleType(), True),\n",
    "    StructField(\"claim_status\", StringType(), True),\n",
    "    StructField(\"denial_reason\", StringType(), True),\n",
    "    StructField(\"created_date\", TimestampType(), True),\n",
    "    StructField(\"updated_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "claim_details_schema = StructType([\n",
    "    StructField(\"claim_detail_id\", StringType(), False),\n",
    "    StructField(\"claim_id\", StringType(), True),\n",
    "    StructField(\"service_date\", DateType(), True),\n",
    "    StructField(\"procedure_code\", StringType(), True),\n",
    "    StructField(\"diagnosis_code\", StringType(), True),\n",
    "    StructField(\"charge_amount\", DoubleType(), True),\n",
    "    StructField(\"units\", IntegerType(), True),\n",
    "    StructField(\"modifier\", StringType(), True),\n",
    "    StructField(\"revenue_code\", StringType(), True),\n",
    "    StructField(\"created_date\", TimestampType(), True),\n",
    "    StructField(\"updated_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Sample data lists for generation\n",
    "insurance_companies = [\n",
    "    (\"Blue Cross Blue Shield\", \"PPO\", \"Commercial\"),\n",
    "    (\"Aetna\", \"HMO\", \"Commercial\"),\n",
    "    (\"UnitedHealthcare\", \"EPO\", \"Commercial\"),\n",
    "    (\"Cigna\", \"POS\", \"Commercial\"),\n",
    "    (\"Humana\", \"HDHP\", \"Commercial\"),\n",
    "    (\"Medicare\", \"Part A\", \"Government\"),\n",
    "    (\"Medicare\", \"Part B\", \"Government\"),\n",
    "    (\"Medicaid\", \"Standard\", \"Government\"),\n",
    "    (\"Tricare\", \"Prime\", \"Government\"),\n",
    "    (\"Kaiser Permanente\", \"HMO\", \"Commercial\")\n",
    "]\n",
    "\n",
    "provider_types = [\n",
    "    \"Hospital\", \"Primary Care\", \"Specialist\", \"Urgent Care\", \n",
    "    \"Outpatient Facility\", \"Laboratory\", \"Imaging Center\", \n",
    "    \"Pharmacy\", \"Home Health\", \"Skilled Nursing Facility\"\n",
    "]\n",
    "\n",
    "specialties = [\n",
    "    \"Family Medicine\", \"Internal Medicine\", \"Pediatrics\", \"Cardiology\",\n",
    "    \"Dermatology\", \"Endocrinology\", \"Gastroenterology\", \"Neurology\",\n",
    "    \"Obstetrics and Gynecology\", \"Oncology\", \"Ophthalmology\", \"Orthopedics\",\n",
    "    \"Psychiatry\", \"Pulmonology\", \"Radiology\", \"Urology\", \"General Surgery\"\n",
    "]\n",
    "\n",
    "# ICD-10 Diagnosis codes (sample)\n",
    "diagnosis_codes = [\n",
    "    \"I10\", \"E11.9\", \"J44.9\", \"F33.1\", \"M54.5\", \"K21.9\", \n",
    "    \"J45.909\", \"I25.10\", \"N18.9\", \"G47.00\", \"M17.9\", \"E78.5\", \n",
    "    \"J06.9\", \"R10.9\", \"Z23\", \"R07.9\", \"H60.501\", \"N39.0\", \"R51\"\n",
    "]\n",
    "\n",
    "# CPT Procedure codes (sample)\n",
    "procedure_codes = [\n",
    "    \"99213\", \"99214\", \"99215\", \"99203\", \"99204\", \"99205\", \n",
    "    \"90471\", \"90715\", \"J0696\", \"36415\", \"80053\", \"71045\", \n",
    "    \"93000\", \"20610\", \"29125\", \"G0008\", \"G0009\", \"97110\" \n",
    "]\n",
    "\n",
    "claim_types = [\"Inpatient\", \"Outpatient\", \"Emergency\", \"Office Visit\", \"Laboratory\", \"Imaging\", \"Pharmacy\"]\n",
    "claim_statuses = [\"Submitted\", \"In Process\", \"Pending\", \"Denied\", \"Paid\", \"Appealed\", \"Voided\"]\n",
    "denial_reasons = [\"\", \"Medical necessity\", \"Non-covered service\", \"Duplicate claim\", \"Invalid code\", \"Timely filing\", \"Missing information\"]\n",
    "places_of_service = [\"11\", \"21\", \"22\", \"23\", \"24\", \"31\", \"32\", \"81\"]  # Standard CMS POS codes\n",
    "\n",
    "# Function to generate a random date within range\n",
    "def random_date(start_date, end_date):\n",
    "    time_between = end_date - start_date\n",
    "    days_between = time_between.days\n",
    "    random_days = random.randrange(days_between)\n",
    "    return start_date + datetime.timedelta(days=random_days)\n",
    "\n",
    "# Function to generate patient data\n",
    "def generate_patients(num_patients=1000):\n",
    "    patients = []\n",
    "    \n",
    "    for i in range(1, num_patients + 1):\n",
    "        patient_id = f\"PT{str(i).zfill(6)}\"\n",
    "        gender = random.choice([\"M\", \"F\"])\n",
    "        first_name = fake.first_name_male() if gender == \"M\" else fake.first_name_female()\n",
    "        last_name = fake.last_name()\n",
    "        \n",
    "        # Generate date of birth (between 18 and 90 years old)\n",
    "        dob = random_date(\n",
    "            datetime.date.today() - datetime.timedelta(days=90*365),\n",
    "            datetime.date.today() - datetime.timedelta(days=18*365)\n",
    "        )\n",
    "        \n",
    "        # Generate insurance ID (will be linked to insurance table)\n",
    "        insurance_id = f\"INS{str(random.randint(1, 10)).zfill(4)}\"\n",
    "        \n",
    "        # Generated timestamps for record creation and update\n",
    "        created_date = datetime.datetime.now() - datetime.timedelta(days=random.randint(30, 730))\n",
    "        updated_date = created_date + datetime.timedelta(days=random.randint(0, 30))\n",
    "        \n",
    "        patients.append((\n",
    "            patient_id,\n",
    "            first_name,\n",
    "            last_name,\n",
    "            gender,\n",
    "            dob,\n",
    "            fake.street_address(),\n",
    "            fake.city(),\n",
    "            fake.state_abbr(),\n",
    "            fake.zipcode(),\n",
    "            fake.phone_number(),\n",
    "            fake.email(),\n",
    "            insurance_id,\n",
    "            created_date,\n",
    "            updated_date\n",
    "        ))\n",
    "    \n",
    "    return spark.createDataFrame(patients, patient_schema)\n",
    "\n",
    "# Function to generate provider data\n",
    "def generate_providers(num_providers=100):\n",
    "    providers = []\n",
    "    \n",
    "    for i in range(1, num_providers + 1):\n",
    "        provider_id = f\"PR{str(i).zfill(6)}\"\n",
    "        provider_type = random.choice(provider_types)\n",
    "        specialty = random.choice(specialties) if provider_type in [\"Primary Care\", \"Specialist\"] else \"\"\n",
    "        \n",
    "        # Generate NPI (National Provider Identifier)\n",
    "        npi = str(random.randint(1000000000, 9999999999))\n",
    "        \n",
    "        # Generated timestamps for record creation and update\n",
    "        created_date = datetime.datetime.now() - datetime.timedelta(days=random.randint(30, 1095))\n",
    "        updated_date = created_date + datetime.timedelta(days=random.randint(0, 30))\n",
    "        \n",
    "        providers.append((\n",
    "            provider_id,\n",
    "            fake.company() + \" \" + provider_type,\n",
    "            provider_type,\n",
    "            npi,\n",
    "            specialty,\n",
    "            fake.street_address(),\n",
    "            fake.city(),\n",
    "            fake.state_abbr(),\n",
    "            fake.zipcode(),\n",
    "            fake.phone_number(),\n",
    "            fake.company_email(),\n",
    "            created_date,\n",
    "            updated_date\n",
    "        ))\n",
    "    \n",
    "    return spark.createDataFrame(providers, provider_schema)\n",
    "\n",
    "# Function to generate insurance data\n",
    "def generate_insurance(num_insurance=10):\n",
    "    insurances = []\n",
    "    \n",
    "    for i in range(1, num_insurance + 1):\n",
    "        insurance_id = f\"INS{str(i).zfill(4)}\"\n",
    "        \n",
    "        # Select a random insurance company and plan type\n",
    "        company_info = random.choice(insurance_companies)\n",
    "        company_name = company_info[0]\n",
    "        plan_type = company_info[1]\n",
    "        \n",
    "        # Generated timestamps for record creation and update\n",
    "        created_date = datetime.datetime.now() - datetime.timedelta(days=random.randint(30, 1095))\n",
    "        updated_date = created_date + datetime.timedelta(days=random.randint(0, 30))\n",
    "        \n",
    "        insurances.append((\n",
    "            insurance_id,\n",
    "            company_name,\n",
    "            plan_type,\n",
    "            fake.bothify(text=\"??###????\"),  # Random alphanumeric plan number\n",
    "            fake.phone_number(),\n",
    "            fake.company_email(),\n",
    "            created_date,\n",
    "            updated_date\n",
    "        ))\n",
    "    \n",
    "    return spark.createDataFrame(insurances, insurance_schema)\n",
    "\n",
    "# Function to generate claims data\n",
    "def generate_claims(num_claims=5000, patients_df=None, providers_df=None, insurance_df=None):\n",
    "    if patients_df is None or providers_df is None or insurance_df is None:\n",
    "        raise ValueError(\"Patient, provider, and insurance DataFrames are required\")\n",
    "    \n",
    "    # Collect IDs to use for references\n",
    "    patient_ids = [row.patient_id for row in patients_df.select(\"patient_id\").collect()]\n",
    "    provider_ids = [row.provider_id for row in providers_df.select(\"provider_id\").collect()]\n",
    "    insurance_ids = [row.insurance_id for row in insurance_df.select(\"insurance_id\").collect()]\n",
    "    \n",
    "    claims = []\n",
    "    \n",
    "    for i in range(1, num_claims + 1):\n",
    "        claim_id = f\"CLM{str(i).zfill(7)}\"\n",
    "        patient_id = random.choice(patient_ids)\n",
    "        provider_id = random.choice(provider_ids)\n",
    "        \n",
    "        # Find the patient's insurance ID\n",
    "        patient_row = patients_df.filter(F.col(\"patient_id\") == patient_id).first()\n",
    "        insurance_id = patient_row.insurance_id\n",
    "        \n",
    "        # Generate claim dates\n",
    "        claim_date = random_date(\n",
    "            datetime.date.today() - datetime.timedelta(days=365),\n",
    "            datetime.date.today() - datetime.timedelta(days=1)\n",
    "        )\n",
    "        \n",
    "        claim_type = random.choice(claim_types)\n",
    "        \n",
    "        # For inpatient claims, add admission and discharge dates\n",
    "        if claim_type == \"Inpatient\":\n",
    "            admission_date = claim_date\n",
    "            los = random.randint(1, 10)  # Length of stay\n",
    "            discharge_date = admission_date + datetime.timedelta(days=los)\n",
    "        else:\n",
    "            admission_date = None\n",
    "            discharge_date = None\n",
    "        \n",
    "        # Generate diagnosis and procedure codes\n",
    "        num_dx = random.randint(1, 4)\n",
    "        num_proc = random.randint(1, 3)\n",
    "        \n",
    "        dx_codes = random.sample(diagnosis_codes, num_dx)\n",
    "        proc_codes = random.sample(procedure_codes, num_proc)\n",
    "        \n",
    "        # Generate financial data\n",
    "        base_charge = random.uniform(100, 5000)\n",
    "        if claim_type == \"Inpatient\":\n",
    "            base_charge *= random.uniform(5, 20)\n",
    "        \n",
    "        total_charge = round(base_charge, 2)\n",
    "        copay_amount = round(random.uniform(0, 50), 2)\n",
    "        \n",
    "        # Determine claim status\n",
    "        claim_status = random.choice(claim_statuses)\n",
    "        \n",
    "        # For denied claims, add a denial reason\n",
    "        if claim_status == \"Denied\":\n",
    "            denial_reason = random.choice(denial_reasons[1:])  # Skip the empty reason\n",
    "        else:\n",
    "            denial_reason = \"\"\n",
    "        \n",
    "        # Calculate insurance and patient payments based on status\n",
    "        if claim_status == \"Paid\":\n",
    "            insurance_paid = round(total_charge * random.uniform(0.7, 0.95), 2)\n",
    "            patient_responsibility = round(total_charge - insurance_paid - copay_amount, 2)\n",
    "        elif claim_status == \"Denied\":\n",
    "            insurance_paid = 0\n",
    "            patient_responsibility = round(total_charge, 2)\n",
    "        else:\n",
    "            insurance_paid = 0\n",
    "            patient_responsibility = 0\n",
    "        \n",
    "        # Place of service\n",
    "        place_of_service = random.choice(places_of_service)\n",
    "        \n",
    "        # Generated timestamps for record creation and update\n",
    "        created_date = datetime.datetime.combine(claim_date, datetime.datetime.min.time()) + datetime.timedelta(days=random.randint(1, 5))\n",
    "        updated_date = created_date + datetime.timedelta(days=random.randint(0, 30))\n",
    "        \n",
    "        claims.append((\n",
    "            claim_id,\n",
    "            patient_id,\n",
    "            provider_id,\n",
    "            insurance_id,\n",
    "            claim_date,\n",
    "            admission_date,\n",
    "            discharge_date,\n",
    "            claim_type,\n",
    "            dx_codes,\n",
    "            proc_codes,\n",
    "            place_of_service,\n",
    "            total_charge,\n",
    "            copay_amount,\n",
    "            insurance_paid,\n",
    "            patient_responsibility,\n",
    "            claim_status,\n",
    "            denial_reason,\n",
    "            created_date,\n",
    "            updated_date\n",
    "        ))\n",
    "    \n",
    "    return spark.createDataFrame(claims, claims_schema)\n",
    "\n",
    "# Function to generate claim details\n",
    "def generate_claim_details(claims_df=None):\n",
    "    if claims_df is None:\n",
    "        raise ValueError(\"Claims DataFrame is required\")\n",
    "    \n",
    "    claim_details = []\n",
    "    detail_id_counter = 1\n",
    "    \n",
    "    # For each claim, generate 1 to 5 line items\n",
    "    for claim in claims_df.collect():\n",
    "        claim_id = claim.claim_id\n",
    "        claim_date = claim.claim_date\n",
    "        \n",
    "        # Determine number of line items\n",
    "        num_details = random.randint(1, 5)\n",
    "        \n",
    "        # Get the procedure codes from the claim\n",
    "        procedure_codes_list = claim.procedure_codes\n",
    "        diagnosis_codes_list = claim.diagnosis_codes\n",
    "        \n",
    "        # If we need more procedure codes than we have, use random ones\n",
    "        if num_details > len(procedure_codes_list):\n",
    "            extra_needed = num_details - len(procedure_codes_list)\n",
    "            extra_codes = random.sample(procedure_codes, extra_needed)\n",
    "            procedure_codes_list = procedure_codes_list + extra_codes\n",
    "        \n",
    "        # Create line items for this claim\n",
    "        total_charge = 0\n",
    "        \n",
    "        for j in range(num_details):\n",
    "            claim_detail_id = f\"DTL{str(detail_id_counter).zfill(8)}\"\n",
    "            detail_id_counter += 1\n",
    "            \n",
    "            # Use a procedure code from the claim\n",
    "            procedure_code = procedure_codes_list[j] if j < len(procedure_codes_list) else random.choice(procedure_codes)\n",
    "            \n",
    "            # Use a diagnosis code from the claim\n",
    "            diagnosis_code = diagnosis_codes_list[0] if diagnosis_codes_list else random.choice(diagnosis_codes)\n",
    "            \n",
    "            # Generate units (typically 1, but can be more for certain services)\n",
    "            units = random.randint(1, 3)\n",
    "            \n",
    "            # Generate charge amount\n",
    "            charge_amount = round(random.uniform(50, 500), 2) * units\n",
    "            total_charge += charge_amount\n",
    "            \n",
    "            # Service date (same as claim date or within a few days)\n",
    "            service_date = claim_date + datetime.timedelta(days=random.randint(0, 3))\n",
    "            \n",
    "            # Modifiers and revenue codes\n",
    "            modifier = random.choice([\"\", \"25\", \"59\", \"XU\", \"TC\", \"26\"])\n",
    "            revenue_code = random.choice([\"\", \"0100\", \"0120\", \"0250\", \"0270\", \"0300\", \"0320\", \"0450\", \"0510\", \"0636\"])\n",
    "            \n",
    "            # Generated timestamps for record creation and update\n",
    "            created_date = claim.created_date\n",
    "            updated_date = claim.updated_date\n",
    "            \n",
    "            claim_details.append((\n",
    "                claim_detail_id,\n",
    "                claim_id,\n",
    "                service_date,\n",
    "                procedure_code,\n",
    "                diagnosis_code,\n",
    "                charge_amount,\n",
    "                units,\n",
    "                modifier,\n",
    "                revenue_code,\n",
    "                created_date,\n",
    "                updated_date\n",
    "            ))\n",
    "    \n",
    "    return spark.createDataFrame(claim_details, claim_details_schema)\n",
    "\n",
    "# Generate the data\n",
    "print(\"Generating patient data...\")\n",
    "patients_df = generate_patients(1000)\n",
    "\n",
    "print(\"Generating provider data...\")\n",
    "providers_df = generate_providers(100)\n",
    "\n",
    "print(\"Generating insurance data...\")\n",
    "insurance_df = generate_insurance(10)\n",
    "\n",
    "print(\"Generating claims data...\")\n",
    "claims_df = generate_claims(5000, patients_df, providers_df, insurance_df)\n",
    "\n",
    "print(\"Generating claim details...\")\n",
    "claim_details_df = generate_claim_details(claims_df)\n",
    "\n",
    "\n",
    "print(\"Data generation complete!\")\n",
    "\n",
    "# Save data to Delta tables\n",
    "print(\"Saving to Delta tables...\")\n",
    "patients_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"healthcare_claims.patients\")\n",
    "providers_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"healthcare_claims.providers\")\n",
    "insurance_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"healthcare_claims.insurance\")\n",
    "claims_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"healthcare_claims.claims\")\n",
    "claim_details_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"healthcare_claims.claim_details\")\n",
    "\n",
    "# Sample queries to verify data\n",
    "print(\"\\nSample patients:\")\n",
    "spark.sql(\"SELECT * FROM healthcare_claims.patients LIMIT 5\").show(truncate=False)\n",
    "\n",
    "print(\"\\nClaim counts by status:\")\n",
    "spark.sql(\"\"\"\n",
    "  SELECT claim_status, COUNT(*) as claim_count \n",
    "  FROM healthcare_claims.claims \n",
    "  GROUP BY claim_status \n",
    "  ORDER BY claim_count DESC\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\nTop diagnosis codes:\")\n",
    "spark.sql(\"\"\"\n",
    "  SELECT diagnosis_code, COUNT(*) as frequency \n",
    "  FROM healthcare_claims.claim_details \n",
    "  GROUP BY diagnosis_code \n",
    "  ORDER BY frequency DESC \n",
    "  LIMIT 10\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\nAverage claim amount by claim type:\")\n",
    "spark.sql(\"\"\"\n",
    "  SELECT claim_type, \n",
    "         COUNT(*) as claim_count,\n",
    "         ROUND(AVG(total_charge), 2) as avg_charge \n",
    "  FROM healthcare_claims.claims \n",
    "  GROUP BY claim_type \n",
    "  ORDER BY avg_charge DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdcc7fd8-cd17-4be2-8831-9c500dbfcc79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to Delta tables...\n\nSample patients:\n+----------+----------+---------+------+-------------+-----------------------------+---------------+-----+--------+------------------+----------------------------+------------+--------------------------+--------------------------+\n|patient_id|first_name|last_name|gender|date_of_birth|address                      |city           |state|zip_code|phone             |email                       |insurance_id|created_date              |updated_date              |\n+----------+----------+---------+------+-------------+-----------------------------+---------------+-----+--------+------------------+----------------------------+------------+--------------------------+--------------------------+\n|PT000001  |Mark      |Johnson  |M     |1937-06-26   |32181 Johnson Course Apt. 389|New Jamesside  |MT   |29394   |394.802.6542x351  |howardmaurice@example.com   |INS0005     |2024-05-31 18:05:06.42537 |2024-06-07 18:05:06.42537 |\n|PT000002  |Robert    |Blair    |M     |2001-04-24   |1849 Ray Squares             |North Donnaport|CO   |31013   |664-375-2553      |amandasanchez@example.com   |INS0002     |2023-03-16 18:05:06.426132|2023-04-08 18:05:06.426132|\n|PT000003  |James     |Mayo     |M     |1988-03-20   |503 Linda Locks              |Carlshire      |FM   |94599   |837-767-2423x88496|wendytaylor@example.com     |INS0007     |2025-01-04 18:05:06.426768|2025-01-04 18:05:06.426768|\n|PT000004  |Brandon   |Williams |M     |1954-11-09   |26916 Carlson Mountain       |Tashatown      |TX   |94967   |284-651-4627      |williamrodriguez@example.net|INS0004     |2023-09-07 18:05:06.427336|2023-09-26 18:05:06.427336|\n|PT000005  |Edwin     |Fowler   |M     |1985-08-04   |52880 Burns Creek            |Natashaport    |IA   |08093   |991.417.1822      |allison96@example.org       |INS0004     |2023-04-12 18:05:06.427949|2023-05-04 18:05:06.427949|\n+----------+----------+---------+------+-------------+-----------------------------+---------------+-----+--------+------------------+----------------------------+------------+--------------------------+--------------------------+\n\n\nClaim counts by status:\n+------------+-----------+\n|claim_status|claim_count|\n+------------+-----------+\n|    Appealed|        734|\n|      Voided|        729|\n|      Denied|        725|\n|   Submitted|        719|\n|  In Process|        708|\n|        Paid|        706|\n|     Pending|        679|\n+------------+-----------+\n\n\nTop diagnosis codes:\n+--------------+---------+\n|diagnosis_code|frequency|\n+--------------+---------+\n|         R07.9|      893|\n|         N39.0|      858|\n|       H60.501|      843|\n|         K21.9|      829|\n|           I10|      806|\n|        I25.10|      803|\n|           Z23|      793|\n|         M54.5|      792|\n|         E11.9|      788|\n|         J44.9|      788|\n+--------------+---------+\n\n\nAverage claim amount by claim type:\n+------------+-----------+----------+\n|  claim_type|claim_count|avg_charge|\n+------------+-----------+----------+\n|   Inpatient|        716|  32183.14|\n|     Imaging|        710|   2613.76|\n|  Outpatient|        703|   2602.65|\n|Office Visit|        725|   2564.14|\n|    Pharmacy|        739|   2513.48|\n|   Emergency|        691|   2496.63|\n|  Laboratory|        716|   2451.69|\n+------------+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Save data to Delta tables\n",
    "print(\"Saving to Delta tables...\")\n",
    "patients_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"healthcare_claims.patients\")\n",
    "providers_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"healthcare_claims.providers\")\n",
    "insurance_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"healthcare_claims.insurance\")\n",
    "claims_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"healthcare_claims.claims\")\n",
    "claim_details_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"healthcare_claims.claim_details\")\n",
    "\n",
    "# Sample queries to verify data\n",
    "print(\"\\nSample patients:\")\n",
    "spark.sql(\"SELECT * FROM healthcare_claims.patients LIMIT 5\").show(truncate=False)\n",
    "\n",
    "print(\"\\nClaim counts by status:\")\n",
    "spark.sql(\"\"\"\n",
    "  SELECT claim_status, COUNT(*) as claim_count \n",
    "  FROM healthcare_claims.claims \n",
    "  GROUP BY claim_status \n",
    "  ORDER BY claim_count DESC\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\nTop diagnosis codes:\")\n",
    "spark.sql(\"\"\"\n",
    "  SELECT diagnosis_code, COUNT(*) as frequency \n",
    "  FROM healthcare_claims.claim_details \n",
    "  GROUP BY diagnosis_code \n",
    "  ORDER BY frequency DESC \n",
    "  LIMIT 10\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\nAverage claim amount by claim type:\")\n",
    "spark.sql(\"\"\"\n",
    "  SELECT claim_type, \n",
    "         COUNT(*) as claim_count,\n",
    "         ROUND(AVG(total_charge), 2) as avg_charge \n",
    "  FROM healthcare_claims.claims \n",
    "  GROUP BY claim_type \n",
    "  ORDER BY avg_charge DESC\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Healthcare Source Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}