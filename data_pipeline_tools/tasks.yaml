fetch_failed_run_debug_information:
  name: fetch_failed_run_debug_information
  description: >
    Fetch run debug data from the Dagster Cloud setup about run id {run_id}.
    Extract only the essential details from the debug data that would be used as the initial evidence for the dailure, including the relevant context
    in which the failure happened, as the first step in the RCA process.

  expected_output: >
    Generate a JSON report with the following top-level groups:

    - Failue summary: A short english description for a the RCA director agents and humans desscribing what was the step that failed an why.

    - Failed step: identify the step where the failure occurred. provide essential details, including the time when it failed, how long was the pipeline runing, etc.

    - Failed step details: A list of all log messages that are relevant for the step that failed. Include the exact command(s) that failed and the exact and FULL error(s) that were reported.

    - Asset details: which assets failed to materialize and why. Highlight specific data quality issues (if any) that were highighted during the run.

    - Codebase Reference: Locate the corresponding code for the steps, resources, and assets involved in the run.

    Guidlines:
    - Do not include sensitive information such as passowrds and account IDs.


assert_failure_persists:
  name: assert_failure_persists
  description: >
    Assert that the problems reported in {run_id} still exists. If the failure seems to be caused by a transient system issue then then use the tools available to you to 
    check if the system issue still exists. If this is a data quality or a failed data test, make sure the test still failed when the involved assets .
    Note that in some cases there could have been considerable time passed between when the error was reported and when this RCA process was kicked off.

  expected_output: >
    Generate a JSON report with the following information

    - Type of failure: System issue OR Data issue

    - Issue validation: the result of validating the current staus of the failure.

    - Validation actions: the set of actions you took to validate the current status of the problem.

    Guidlines:
    - Do not include sensitive information such as passowrds and account IDs.

trace_upstream_dependencies:
  name: trace_upstream_dependencies
  description: >
   Trace upstream dependencies of the failed step in {run_id} by looking at the Dagster pipeline lineage. Use asset state, asset metadata and asset lineage
    to trace the root of the prolem as much upsteam as possible, as long as the state of the upstream assets can explain the failure downstream.
    If logs indicate DBT-related failures try to:

    - Fetch DBT logs (manifest.json and run_results.json) for the affected run.

    - Identify failed DBT models or other SQL or python code, associated quality tests, and dependencies.

    If the failure involves custom Python scripts or other transformations, fetch the specific logs or code references for the affected step.
    If the issue involves external data sources (e.g., S3, databases), query the data sources for anomalies (e.g., missing data, schema changes).
  expected_output: >
    A JSON file with the state of all the upstream assets that were inspected:
    - Asset details
    - asset state, pertaining to the failure
    - Source of information


correlate_findings:
  name: correlate_findings
  description: >
    Correlate logs, lineage information, and findings from DBT, Python, or data sources.
    Dedicde if we have sufficient information to compile a helpful and compelling RCA report for the failure in run_id {run_id}. 
    A compelling RCA report requires that we have all the supporting evidence to pin-point root cause of the failure. If more information is needed,
    fetch the required information before proceeding to the next step.

  expected_output: >-
    Describe your chain of throught that leads from the original failure reported to the likely root causes, citing the exact parts of the evidence that supports your reasoning.
    
    


generate_rca_report:
  name: generate_rca_report
  description: >
    Compile all findings into a structured RCA report for Dagster run id {run_id}.

  expected_output: >
    - Reported failure: A concice description the original failure

    - Root cause summary: A list of the likely root causes of the failure.

    - Explanation: Clear explanation of the each of the root causes listed above, including detailed supporting evidence for your claims such as failed commands, log records, status checks, SQL or python code.

    - Recommendations: List any next steps that you'd recommed for resolving the issues. Be as specific and accurate as possible. For example, if recommending to improve test code or query, or improving an asset 
      definition, show an example for such improvments, where relevant.




generate_rca_report_one_shot:
  name: generate_rca_report_one_shot
  description: >
    Generate a root-cause-analysis (RCA) report for run id {run_id}, given the run failure details in your context which was provided to you by analyzing a full run debug file.

    Follow these steps in order to trace the root cause:

    ### Step_1: 
        name: assert_failure_persists
        description: 
            Assert that the problems reported in {run_id} still exists. If the failure seems to be caused by a transient system issue then then use the tools available to you to to check if the system issue still exists. If this is a data quality or a failed data test, make sure the test still failed when the involved assets .
            Note that in some cases there could have been considerable time passed between when the error was reported and when this RCA process was kicked off.

    ### Step_2:
        name: trace_upstream_dependencies
        description:
            Trace upstream dependencies of the failed step in {run_id} by looking at the Dagster pipeline lineage. Use asset state, asset metadata and asset lineage to trace the root of the problem as much upsteam as possible. 
            
            If asset X is an upstream asset of Y, examine the data in asset X to see if it could potentially 
            explain the problem in Y. For example, if a data check failed (like non-null or no-duplicates) for a column in asset Y, 
            examine the definition of asset Y to determine what query to run against asset X to check for the source of that data item.
            If it turns out that indeed the data in X explains the data check in Y, repeat this step for all upstream assets of X, recurisvely.
            REMEMBER - your goal is to trace the ROOT of the problem as much upstream as possible.
              
            If the failure involves custom Python scripts or other transformations, fetch the specific logs or code references for the affected step.
            If the issue involves external data sources (e.g., S3, databases), use your available tools to query the data sources for the relevant data or state (e.g. null,  missing data, duplicate data, schema changes).      

    ### Step_3:
        name: correlate_findings
        description:
            Correlate logs, lineage information, and findings from DBT, Python, or relevant data sources.
            Dedicde if we have sufficient information to compile a helpful and compelling RCA report for the failure in run_id {run_id}. 
            A compelling RCA report requires that we have all the supporting evidence to pin-point root cause of the failure. If more information is needed,
            fetch the required information before proceeding to the next step.

    ### Step_4:
        name: generate_rca_report
        description: Compile all findings into a structured RCA report for Dagster run id {run_id}.


  expected_output: >
    A detaied root cause analysis report, with the following outline:

    Root Cause Analysis Report: Data Pipeline Failure
    1. Executive Summary
        A brief overview of the incident, including the time of failure, impact, and the importance of this RCA.
    2. Incident Description
        Date and Time of Incident: When the failure was first detected.
        Context: Overview of the affected pipeline, including its primary function and key stakeholders.
        Observed Symptoms: Specific symptoms or errors that indicated failure (e.g., job timeouts, data discrepancies, missing outputs).
    3. Impact Assessment
        Business Impact: A clear description of how the failure affected the business or customers.
        Data Impact: Scope and extent of the data affected (e.g., delayed reports, corrupted or incomplete data).
        Affected Systems: Systems and components impacted directly or indirectly by the failure.
    4. Timeline of Events
        Incident Timeline: Step-by-step account of the incident, from detection to resolution.
        Key Actions and Responses: Details of actions taken by team members to mitigate and investigate the issue.
        Detection and Notification: How and when the issue was detected (e.g., monitoring alerts, user reports).
    5. Root Cause
        Technical Analysis: A detailed breakdown of what caused the failure, including technical evidence.
        Underlying Issues: Identification of contributing factors (e.g., configuration error, code bug, dependency failure).
        Tools Used: Any tools or methodologies used to trace the root cause (e.g., log analysis, metrics monitoring).
    6. Contributing Factors
        Environmental Factors: Anything external that contributed (e.g., network issues, third-party service failures).
        Process Gaps: Gaps in operational processes that may have allowed or exacerbated the failure (e.g., insufficient testing, unclear escalation paths).
    7. Resolution and Recovery
        Mitigation Actions: Steps taken to restore services and ensure data accuracy.
        Recovery Timeline: Details on how long it took to recover and restore the data pipeline.
        Verification and Validation: Measures taken to confirm the problem was resolved and data integrity was restored.
        8. Preventive Measures and Recommendations
        Short-term Fixes: Quick mitigations implemented to prevent recurrence.
        Long-term Preventive Actions: Planned improvements (e.g., code changes, additional testing).
        Monitoring Improvements: Changes to monitoring and alerting to improve early detection.
        Training/Process Updates: Changes to team processes or additional training required.

    10. Follow-Up Actions
        Action Items List: A clear list of follow-up tasks, assigned owners, and target deadlines.
        Status Tracking: Methods for tracking progress on preventive measures.
    11. Appendices
        Logs and Evidence: Attach relevant logs, screenshots, or other data analyzed.
        Technical Diagrams: Include pipeline architecture diagrams, where applicable, for better understanding

            