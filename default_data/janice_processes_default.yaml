janice_monitor_unused_tables:
  BOT_ID: "Janice"
  PROCESS_NAME: "janice_monitor_unused_tables"
  PROCESS_INSTRUCTIONS: |
    # Monitor Unused Data Tables

    ## Overview
    This process identifies the top 10 largest unused tables across all databases and notifies stakeholders about cost-saving opportunities involving unused tables in Snowflake.

    ## Process Steps:
    1. **Identify The Top 10 Largest Unused Tables Across All Databases:**
       - Execute the following SQL query to identify the 10 largest tables across all databases and schemas in the Snowflake account: `CALL GENESIS_BOTS_ALPHA.APP1.run_dynamic_sql('QUERY_50');`
       - Confirm with the supervisor that you have run the query and note the number of rows you received in response.

    2. **Draft a Compelling Email:**
       - Draft an email with a catchy subject line including Unicode emojis about a cost savings opportunity involving unused tables in Snowflake :bar_chart::moneybag:.
       - Include in the email body:
         - An explanation of what an unused table is.
         - A list of the 3 biggest unused tables.
         - Mention that there are more unused tables.
         - Invite the user to come talk with you, Janice, in Streamlit or Slack by mentioning the Process ID.
         - Be sure to include line breaks in the body where appropriate, using the standard line break indicator (`\n`).

    3. **Send the Email:**
       - Send the email to `SYS$DEFAULT_EMAIL`.

    ## Conclusion
    This process helps in identifying unused data tables and communicates cost-saving opportunities to stakeholders effectively.

    ## Example Email Output Structure
    - Subject: (Catchy subject line with emojis)
    - Body: (Email body following the specified guidelines)
  PROCESS_DETAILS: "Monitor Unused Data Tables"
  TIMESTAMP: "2000-01-01T00:00:00.000Z"

  janice_top_warehouses_spillage:
    BOT_ID: "Janice"
    PROCESS_NAME: "janice_top_warehouses_spillage"
    PROCESS_INSTRUCTIONS: |
      # Top 3 Warehouses Affected by Spillage and Queuing

      ## Overview
      This process identifies the top 3 warehouses with data spilling issues and analyzes average queuing times per month in the Snowflake environment. It aims to notify stakeholders about performance issues and opportunities to optimize warehouse configurations.

      ## Process Steps:

      1. **Identify The Top 3 Warehouses With Data Spilling:**
        - Execute the following SQL query to identify the top 3 warehouses with data spilling in the Snowflake account:
          ```sql
          CALL GENESIS_BOTS_ALPHA.APP1.RUN_PROCESS_SQL('QUERY_52');
          
        - Confirm with the supervisor that you have run the query and note the number of rows you received in response.
      2. **Average Queuing Times Per Month:**
        - Execute the following SQL query to identify the average queuing times per month:
          
      sql
          CALL GENESIS_BOTS_ALPHA.APP1.RUN_PROCESS_SQL('QUERY_53');
          
        - Confirm with the supervisor that you have run the query and note the number of rows you received in response.
      3. **Draft a Compelling Email:**
        - Draft an email with a catchy subject line including Unicode emojis about identified performance issues and an opportunity to optimize warehouse configurations in their Snowflake Environment (e.g., :snowflake::hammer_and_wrench:).
        - Include in the email body:
          - An explanation of what it means when a warehouse has bytes spilled to local and remote storage.
          - An explanation of how average queuing time impacts performance.
          - A list of the top 3 warehouses that are spilling to local and remote storage along with their average queue times from your analysis.
          - Mention that there may be more warehouses experiencing spillage.
          - Invite the user to come talk with you, Janice, in Streamlit or Slack by mentioning the Process ID.
          - Ensure line breaks are included in the body where appropriate, using the standard line break indicator (\n).

      4. **Send the Email:**
        - Send the email to SYS$DEFAULT_EMAIL.

      ## Conclusion
      This process helps in identifying performance issues related to data spillage and queuing times, enabling stakeholders to optimize warehouse configurations for better performance.

      ## Example Email Output Structure
      - **Subject:** :snowflake: Performance Alert: Warehouse Optimization Needed :hammer_and_wrench:
      - **Body:**
        
        Hello,

        We've identified performance issues in our Snowflake environment related to data spillage and queuing times. When a warehouse spills bytes to local and remote storage, it indicates that the warehouse doesn't have sufficient memory to process queries efficiently, leading to slower performance.

        Average queuing time impacts how long queries wait before being executed. High queuing times mean that queries are delayed due to resource constraints, affecting overall system responsiveness.

        Here are the top 3 warehouses experiencing these issues:
        1. [Warehouse_Name_1] - Spillage: [Amount], Average Queue Time: [Duration]
        2. [Warehouse_Name_2] - Spillage: [Amount], Average Queue Time: [Duration]
        3. [Warehouse_Name_3] - Spillage: [Amount], Average Queue Time: [Duration]

        There may be more warehouses experiencing spillage and queuing issues.

        Please reach out to me, Janice, in Streamlit or Slack to discuss this further. Mention Process ID: [PROCESS_ID].

        Regards,
        Janice
        
    PROCESS_DETAILS: "Top 3 Warehouses Affected by Spillage and Queuing"
    TIMESTAMP: "2000-01-01T00:00:00.000Z"